{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "corenlp_all_i_need.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Munima/NLP-tasks/blob/master/corenlp_all_i_need.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m38OMkA1wiZk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Mounting googlr drive\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDlaHJq5IO3p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "0e24ac35-b177-4d4e-adc2-b836f1284f43"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsOQ0thGIfDH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "outputId": "b3387df2-2aed-4aff-8ceb-f1e309d2e191"
      },
      "source": [
        "# Install stanfordnlp; note that the prefix \"!\" is not needed if you are running in a terminal\n",
        "!pip install stanfordnlp\n",
        "\n",
        "# Import stanfordnlp\n",
        "import stanfordnlp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting stanfordnlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/bf/5d2898febb6e993fcccd90484cba3c46353658511a41430012e901824e94/stanfordnlp-0.2.0-py3-none-any.whl (158kB)\n",
            "\r\u001b[K     |██                              | 10kB 18.7MB/s eta 0:00:01\r\u001b[K     |████▏                           | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 30kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 61kB 2.5MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 71kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 81kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 92kB 3.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 163kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (1.17.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (4.28.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (2.21.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (3.10.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (1.3.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (2019.11.28)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->stanfordnlp) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->stanfordnlp) (42.0.2)\n",
            "Installing collected packages: stanfordnlp\n",
            "Successfully installed stanfordnlp-0.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lx0tN1E9IhxS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "668753fc-3c61-4fb8-faa7-529a0d296053"
      },
      "source": [
        "# Download the Stanford CoreNLP Java library and unzip it to a ./corenlp folder\n",
        "!echo \"Downloading CoreNLP...\"\n",
        "!wget \"http://nlp.stanford.edu/software/stanford-corenlp-full-2018-10-05.zip\" -O corenlp.zip\n",
        "!unzip corenlp.zip\n",
        "!mv ./stanford-corenlp-full-2018-10-05 ./corenlp\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading CoreNLP...\n",
            "--2019-12-11 03:10:06--  http://nlp.stanford.edu/software/stanford-corenlp-full-2018-10-05.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/software/stanford-corenlp-full-2018-10-05.zip [following]\n",
            "--2019-12-11 03:10:07--  https://nlp.stanford.edu/software/stanford-corenlp-full-2018-10-05.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 393239982 (375M) [application/zip]\n",
            "Saving to: ‘corenlp.zip’\n",
            "\n",
            "corenlp.zip         100%[===================>] 375.02M  13.8MB/s    in 28s     \n",
            "\n",
            "2019-12-11 03:10:35 (13.5 MB/s) - ‘corenlp.zip’ saved [393239982/393239982]\n",
            "\n",
            "Archive:  corenlp.zip\n",
            "   creating: stanford-corenlp-full-2018-10-05/\n",
            "  inflating: stanford-corenlp-full-2018-10-05/jaxb-core-2.3.0.1-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/xom-1.2.10-src.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/CoreNLP-to-HTML.xsl  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/README.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/jollyday-0.4.9-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/LIBRARY-LICENSES  \n",
            "   creating: stanford-corenlp-full-2018-10-05/sutime/\n",
            "  inflating: stanford-corenlp-full-2018-10-05/sutime/british.sutime.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/sutime/defs.sutime.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/sutime/spanish.sutime.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/sutime/english.sutime.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/sutime/english.holidays.sutime.txt  \n",
            " extracting: stanford-corenlp-full-2018-10-05/ejml-0.23-src.zip  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/input.txt.xml  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/build.xml  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/pom.xml  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/stanford-corenlp-3.9.2-javadoc.jar  \n",
            "   creating: stanford-corenlp-full-2018-10-05/tokensregex/\n",
            "  inflating: stanford-corenlp-full-2018-10-05/tokensregex/color.input.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/tokensregex/retokenize.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/tokensregex/color.properties  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/tokensregex/color.rules.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/javax.json-api-1.0-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/jaxb-api-2.4.0-b180830.0359-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/stanford-corenlp-3.9.2-models.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/protobuf.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/javax.activation-api-1.2.0.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/StanfordDependenciesManual.pdf  \n",
            "   creating: stanford-corenlp-full-2018-10-05/patterns/\n",
            "  inflating: stanford-corenlp-full-2018-10-05/patterns/example.properties  \n",
            " extracting: stanford-corenlp-full-2018-10-05/patterns/otherpeople.txt  \n",
            " extracting: stanford-corenlp-full-2018-10-05/patterns/goldplaces.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/patterns/stopwords.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/patterns/presidents.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/patterns/names.txt  \n",
            " extracting: stanford-corenlp-full-2018-10-05/patterns/places.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/patterns/goldnames.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/slf4j-simple.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/stanford-corenlp-3.9.2-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/input.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/joda-time.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/xom.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/jaxb-impl-2.4.0-b180830.0438-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/StanfordCoreNlpDemo.java  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/jaxb-core-2.3.0.1.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/RESOURCE-LICENSES  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/javax.activation-api-1.2.0-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/slf4j-api.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/pom-java-11.xml  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/ejml-0.23.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/javax.json.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/Makefile  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/corenlp.sh  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/joda-time-2.9-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/jaxb-api-2.4.0-b180830.0359.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/jollyday.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/ShiftReduceDemo.java  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/jaxb-impl-2.4.0-b180830.0438.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/stanford-corenlp-3.9.2.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/SemgrexDemo.java  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/LICENSE.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDsKgcjOGqOd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the CORENLP_HOME environment variable to point to the installation location\n",
        "import os\n",
        "os.environ[\"CORENLP_HOME\"] = \"./corenlp\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nblhzgm8IzSE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import client module\n",
        "from stanfordnlp.server import CoreNLPClient"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sd59-Vs5I6BR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "965316b6-9aea-4be8-aab7-326a6d6397fe"
      },
      "source": [
        "# Construct a CoreNLPClient with some basic annotators, a memory allocation of 4GB, and port number 9001\n",
        "client = CoreNLPClient(properties={'annotators': 'coref', 'coref.algorithm' : 'statistical'},annotators=['tokenize','ssplit', 'pos', 'lemma', 'ner','coref'], memory='4G', endpoint='http://localhost:9001',timeout=10000)\n",
        "print(client)\n",
        "\n",
        "# Start the background server and wait for some time\n",
        "# Note that in practice this is totally optional, as by default the server will be started when the first annotation is performed\n",
        "client.start()\n",
        "import time; time.sleep(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<stanfordnlp.server.client.CoreNLPClient object at 0x7f64f9d1c550>\n",
            "Starting server with command: java -Xmx4G -cp ./corenlp/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9001 -timeout 10000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-7227874323f8435c.props -preload tokenize,ssplit,pos,lemma,ner,coref\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVQD2L9UgE7T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "234c0ff6-9111-4c00-efcc-7d63ad9527cb"
      },
      "source": [
        "#text = \"Barack Obama was not born in Hawaii.He was born in Newyork.\"\n",
        "text=\"The system should be able to return a product before processing the credentials of the user \\\n",
        " who is placing the order on the same day that the order is placed.\"\n",
        "#text=\"The musical store receives tape requests from customers.\"\n",
        "#text=\"System verifies login information for candidate\"\n",
        "# set up the client\n",
        "#with CoreNLPClient(annotators=['tokenize','ssplit','pos','lemma','ner', 'depparse'], timeout=60000, memory='16G') as client:\n",
        "# submit the request to the server\n",
        "ann = client.annotate(text)\n",
        "\n",
        "offset = 0 # keeps track of token offset for each sentence\n",
        "for sentence in ann.sentence:\n",
        "    print('___________________')\n",
        "    print('dependency parse:')\n",
        "    # extract dependency parse\n",
        "    dp = sentence.basicDependencies\n",
        "    # build a helper dict to associate token index and label\n",
        "    token_dict = {sentence.token[i].tokenEndIndex-offset : sentence.token[i].word for i in range(0, len(sentence.token))}\n",
        "    offset += len(sentence.token)\n",
        "\n",
        "    # build list of (source, target) pairs\n",
        "    out_parse = [(dp.edge[i].source, dp.edge[i].target) for i in range(0, len(dp.edge))]\n",
        "\n",
        "    for source, target in out_parse:\n",
        "        print(source, token_dict[source], '->', target, token_dict[target])\n",
        "\n",
        "    print('\\nTokens \\t POS \\t NER')\n",
        "    for token in sentence.token:\n",
        "        print (token.word, '\\t', token.pos, '\\t', token.ner)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "___________________\n",
            "dependency parse:\n",
            "2 system -> 1 The\n",
            "5 able -> 2 system\n",
            "5 able -> 3 should\n",
            "5 able -> 4 be\n",
            "5 able -> 7 return\n",
            "5 able -> 31 .\n",
            "7 return -> 6 to\n",
            "7 return -> 9 product\n",
            "7 return -> 11 processing\n",
            "9 product -> 8 a\n",
            "11 processing -> 10 before\n",
            "11 processing -> 13 credentials\n",
            "13 credentials -> 16 user\n",
            "13 credentials -> 12 the\n",
            "16 user -> 19 placing\n",
            "16 user -> 14 of\n",
            "16 user -> 15 the\n",
            "19 placing -> 17 who\n",
            "19 placing -> 18 is\n",
            "19 placing -> 21 order\n",
            "19 placing -> 25 day\n",
            "19 placing -> 30 placed\n",
            "21 order -> 20 the\n",
            "25 day -> 22 on\n",
            "25 day -> 23 the\n",
            "25 day -> 24 same\n",
            "28 order -> 27 the\n",
            "30 placed -> 26 that\n",
            "30 placed -> 28 order\n",
            "30 placed -> 29 is\n",
            "\n",
            "Tokens \t POS \t NER\n",
            "The \t DT \t O\n",
            "system \t NN \t O\n",
            "should \t MD \t O\n",
            "be \t VB \t O\n",
            "able \t JJ \t O\n",
            "to \t TO \t O\n",
            "return \t VB \t O\n",
            "a \t DT \t O\n",
            "product \t NN \t O\n",
            "before \t IN \t O\n",
            "processing \t VBG \t O\n",
            "the \t DT \t O\n",
            "credentials \t NNS \t O\n",
            "of \t IN \t O\n",
            "the \t DT \t O\n",
            "user \t NN \t O\n",
            "who \t WP \t O\n",
            "is \t VBZ \t O\n",
            "placing \t VBG \t O\n",
            "the \t DT \t O\n",
            "order \t NN \t O\n",
            "on \t IN \t O\n",
            "the \t DT \t DATE\n",
            "same \t JJ \t DATE\n",
            "day \t NN \t DATE\n",
            "that \t IN \t O\n",
            "the \t DT \t O\n",
            "order \t NN \t O\n",
            "is \t VBZ \t O\n",
            "placed \t VBN \t O\n",
            ". \t . \t O\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTl6q0X7SgtV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ffc0c3ab-8ed0-43ac-ad4a-4f83a61cb353"
      },
      "source": [
        "#def depparse(text):\n",
        "#text=\"Chris Manning is a nice person. Chris wrote a simple sentence. He also gives oranges to people.\"\n",
        "#text = \"Barack Obama was not born in Hawaii. He was born in Newyork.\"\n",
        "parsed=\"\"\n",
        "nlp=client\n",
        "\n",
        "output = nlp.annotate(text, properties={\n",
        "'annotators': 'tokenize,ssplit,pos,depparse,parse',\n",
        "\"timeout\": \"50000\",\n",
        "'outputFormat': 'json'\n",
        "\n",
        " })\n",
        "for i in output['sentences']: # not sure if there can be multiple items here. If so, it just returns the first one currently.\n",
        "  for dep in i['basicDependencies']:\n",
        "    print(dep['dep'],dep['governorGloss'],dep['governor'])\n",
        "  #print( [tuple((dep['dep'], dep['governorGloss'], dep['dependentGloss'])) for dep in i['basicDependencies']])\n",
        "print(output['sentences'][0]['parse'])\n",
        "print('------------------------------')\n",
        "#print(output['sentences'][1]['parse'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROOT ROOT 0\n",
            "det system 2\n",
            "nsubj able 5\n",
            "aux able 5\n",
            "cop able 5\n",
            "mark return 7\n",
            "xcomp able 5\n",
            "det product 9\n",
            "dobj return 7\n",
            "mark processing 11\n",
            "advcl return 7\n",
            "det credentials 13\n",
            "dobj processing 11\n",
            "case user 16\n",
            "det user 16\n",
            "nmod credentials 13\n",
            "nsubj placing 19\n",
            "aux placing 19\n",
            "acl:relcl user 16\n",
            "det order 21\n",
            "dobj placing 19\n",
            "case day 25\n",
            "det day 25\n",
            "amod day 25\n",
            "nmod order 21\n",
            "mark placed 30\n",
            "det order 28\n",
            "nsubjpass placed 30\n",
            "auxpass placed 30\n",
            "ccomp able 5\n",
            "punct able 5\n",
            "(ROOT\n",
            "  (S\n",
            "    (NP (DT The) (NN system))\n",
            "    (VP (MD should)\n",
            "      (VP (VB be)\n",
            "        (ADJP (JJ able)\n",
            "          (S\n",
            "            (VP (TO to)\n",
            "              (VP (VB return)\n",
            "                (NP (DT a) (NN product))\n",
            "                (PP (IN before)\n",
            "                  (S\n",
            "                    (VP (VBG processing)\n",
            "                      (NP\n",
            "                        (NP (DT the) (NNS credentials))\n",
            "                        (PP (IN of)\n",
            "                          (NP\n",
            "                            (NP (DT the) (NN user))\n",
            "                            (SBAR\n",
            "                              (WHNP (WP who))\n",
            "                              (S\n",
            "                                (VP (VBZ is)\n",
            "                                  (VP (VBG placing)\n",
            "                                    (NP\n",
            "                                      (NP (DT the) (NN order))\n",
            "                                      (PP (IN on)\n",
            "                                        (NP (DT the) (JJ same) (NN day))))))))))))))))))\n",
            "        (SBAR (IN that)\n",
            "          (S\n",
            "            (NP (DT the) (NN order))\n",
            "            (VP (VBZ is)\n",
            "              (ADJP (VBN placed)))))))\n",
            "    (. .)))\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9_lwPCGcY1_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "55a48d40-00bd-47fe-bc4c-19936ccf80bf"
      },
      "source": [
        "nlp=client\n",
        "output = nlp.semgrex(text, pattern='{tag: VBN}', filter=False)\n",
        "print(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'sentences': [{'0': {'text': 'placed', 'begin': 29, 'end': 30}, 'length': 1}]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bm5zIqsQea9N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "outputId": "41a58192-2985-4216-9cb7-a6693aff92e7"
      },
      "source": [
        "#text=\"System verifies login information for user.\"\n",
        "text=\"The musical store receives tape requests from customers. The musical store receives new tapes from the Main office.\"\n",
        "output = nlp.annotate(\n",
        "        text, properties={\"annotators\": \"tokenize,ssplit,pos,depparse,parse,ner\", \"outputFormat\": \"json\"}\n",
        ")\n",
        "# pprint.pprint(output)\n",
        "#tree = output[\"sentences\"][0][\"parse\"]\n",
        "#print(tree)\n",
        "l=len(output[\"sentences\"])\n",
        "for j in range(l):\n",
        "  x = output[\"sentences\"][j][\"basicDependencies\"]\n",
        "  # pprint.pprint(x)\n",
        "  print (\"-------------------------------------------------\")\n",
        "  for i in range(len(x)):\n",
        "      print (x[i][\"dep\"] + \"-->\" + x[i][\"governorGloss\"] + \"-\" + str(x[i][\"governor\"]) + \" \" + x[i][\n",
        "          \"dependentGloss\"\n",
        "      ] + \"-\" + str(x[i][\"dependent\"]))\n",
        "  for i in range(len(x)):\n",
        "    if x[i][\"dep\"]=='ROOT':\n",
        "      msg=str(x[i][\"dependentGloss\"])\n",
        "    elif x[i][\"dep\"]=='nsubj':\n",
        "      send=str(x[i][\"dependentGloss\"])\n",
        "    elif x[i][\"dep\"]=='dobj':\n",
        "      msg1=str(x[i][\"dependentGloss\"])\n",
        "    elif x[i][\"dep\"]=='nmod':\n",
        "      rec=str(x[i][\"dependentGloss\"])\n",
        "  print(\"-------------------------------\")\n",
        "  print (send + \"-->\" + msg+ \" \"+msg1+\"-->\" + rec) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------------------------------------\n",
            "ROOT-->ROOT-0 receives-4\n",
            "det-->store-3 The-1\n",
            "amod-->store-3 musical-2\n",
            "nsubj-->receives-4 store-3\n",
            "compound-->requests-6 tape-5\n",
            "dobj-->receives-4 requests-6\n",
            "case-->customers-8 from-7\n",
            "nmod-->receives-4 customers-8\n",
            "punct-->receives-4 .-9\n",
            "-------------------------------\n",
            "store-->receives requests-->customers\n",
            "-------------------------------------------------\n",
            "ROOT-->ROOT-0 receives-4\n",
            "det-->store-3 The-1\n",
            "amod-->store-3 musical-2\n",
            "nsubj-->receives-4 store-3\n",
            "amod-->tapes-6 new-5\n",
            "dobj-->receives-4 tapes-6\n",
            "case-->office-10 from-7\n",
            "det-->office-10 the-8\n",
            "compound-->office-10 Main-9\n",
            "nmod-->receives-4 office-10\n",
            "punct-->receives-4 .-11\n",
            "-------------------------------\n",
            "store-->receives tapes-->office\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXs9WrAvkyKO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def typedependencies(sent_list, neg_words, compound_word_list):\n",
        "\n",
        "    pos_dict = {}\n",
        "    depend_dict = {}\n",
        "    depend_list = []\n",
        "    proper_names = []\n",
        "    # neg_words = []\n",
        "    compound_dic = {}\n",
        "\n",
        "    #nlp = StanfordCoreNLP(\"http://localhost:9000\")\n",
        "    nlp=client\n",
        "    for i in range(len(sent_list)):\n",
        "        compound_list = []\n",
        "        print(sent_list[i])\n",
        "        output = nlp.annotate(\n",
        "            sent_list[i], properties={\"annotators\": \"tokenize,ssplit,pos,depparse,parse,ner\", \"outputFormat\": \"json\"}\n",
        "        )\n",
        "        # pprint.pprint(output)\n",
        "        x = output[\"sentences\"][0][\"basicDependencies\"]\n",
        "        # pprint.pprint(output['sentences'][0]['parse'])\n",
        "        # pprint.pprint(x)\n",
        "        # print '-------------------------------------------------'\n",
        "        for j in range(len(x)):\n",
        "\n",
        "            if \"compound\" in x[j][\"dep\"]:\n",
        "                # compound_word(x[j])\n",
        "                ll = [x[j][\"governorGloss\"], x[j][\"governor\"], x[j][\"dependentGloss\"], x[j][\"dependent\"]]\n",
        "                compound_dic[x[j][\"governor\"]] = x[j][\"governorGloss\"]\n",
        "                compound_dic[x[j][\"dependent\"]] = x[j][\"dependentGloss\"]\n",
        "                # compound_list.append(ll)\n",
        "\n",
        "            d = [\n",
        "                x[j][\"dep\"],\n",
        "                x[j][\"governorGloss\"],\n",
        "                str(x[j][\"governor\"]),\n",
        "                x[j][\"dependentGloss\"],\n",
        "                str(x[j][\"dependent\"]),\n",
        "            ]\n",
        "            depend_list.append(d)\n",
        "\n",
        "            # getting the negative words..\n",
        "            if \"neg\" in x[j][\"dep\"]:\n",
        "                x1 = x[j][\"governorGloss\"].lower()\n",
        "                x2 = x[j][\"dependentGloss\"].lower()\n",
        "                if x1 not in stopwords:\n",
        "                    neg_words.append([x1, x[j][\"governor\"]])\n",
        "                else:\n",
        "                    neg_words.append([x2, x[j][\"dependent\"]])\n",
        "\n",
        "            if \"conj\" in x[j][\"dep\"]:\n",
        "                x1 = x[j][\"governorGloss\"].lower()\n",
        "                x2 = x[j][\"dependentGloss\"].lower()\n",
        "                if x1 in neg_prefix:\n",
        "                    neg_words.append([x2, x[j][\"dependent\"]])\n",
        "                # elif (x2 == 'not' or x2 == 'nor' or x2 == 'non'):\n",
        "                #   neg_words.append(x1)\n",
        "                elif x2 in neg_prefix:\n",
        "                    neg_words.append([x1, x[j][\"governor\"]])\n",
        "\n",
        "            print (\n",
        "                x[j][\"dep\"]\n",
        "                + \"-->\"\n",
        "                + x[j][\"governorGloss\"]\n",
        "                + \"-\"\n",
        "                + str(x[j][\"governor\"])\n",
        "                + \" \"\n",
        "                + x[j][\"dependentGloss\"]\n",
        "                + \"-\"\n",
        "                + str(x[j][\"dependent\"])\n",
        "            )\n",
        "        print (\"===================================\")\n",
        "\n",
        "        for key, value in sorted(compound_dic.items()):\n",
        "            compound_list.append([key, value])\n",
        "        # print compound_word(compound_list)\n",
        "        compound_dic.clear()\n",
        "\n",
        "        y = output[\"sentences\"][0][\"tokens\"]\n",
        "        for k in range(len(y)):\n",
        "            pos_dict[y[k][\"word\"]] = y[k][\"pos\"]\n",
        "            if \"NNP\" in y[k][\"pos\"]:\n",
        "                proper_names.append(y[k][\"word\"])\n",
        "\n",
        "        depend_dict[i] = depend_list\n",
        "        depend_list = []\n",
        "\n",
        "        if len(compound_list) > 0:\n",
        "            w = compound_word(compound_list)\n",
        "        else:\n",
        "            w = []\n",
        "        for jj in range(len(w)):\n",
        "            if w[jj] != \"\":\n",
        "                print (w[jj])\n",
        "                compound_word_list.append(w[jj])\n",
        "\n",
        "    print (\"--------NAMES------\" + str(proper_names))\n",
        "    print (\"--------NEGATIVE----\" + str(neg_words))\n",
        "    return depend_dict, pos_dict, proper_names\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daghOSTelxrv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "outputId": "7e16bffa-4b2e-4255-e60e-03f5db312fa3"
      },
      "source": [
        "\n",
        "text =list(\"Barack Obama was not born in Hawaii. He was born in Newyork.\")\n",
        "typedependencies(text, \"\", \"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "B\n",
            "ROOT-->ROOT-0 B-1\n",
            "===================================\n",
            "a\n",
            "ROOT-->ROOT-0 a-1\n",
            "===================================\n",
            "r\n",
            "ROOT-->ROOT-0 r-1\n",
            "===================================\n",
            "a\n",
            "ROOT-->ROOT-0 a-1\n",
            "===================================\n",
            "c\n",
            "ROOT-->ROOT-0 c-1\n",
            "===================================\n",
            "k\n",
            "ROOT-->ROOT-0 k-1\n",
            "===================================\n",
            " \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-104-1536736a015d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Barack Obama was not born in Hawaii. He was born in Newyork.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtypedependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-103-ea37e43558cf>\u001b[0m in \u001b[0;36mtypedependencies\u001b[0;34m(sent_list, neg_words, compound_word_list)\u001b[0m\n\u001b[1;32m     17\u001b[0m         )\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# pprint.pprint(output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sentences\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"basicDependencies\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;31m# pprint.pprint(output['sentences'][0]['parse'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# pprint.pprint(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPB33zK1gc7j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a26ee843-312e-48f4-d5ab-02ea94afc88c"
      },
      "source": [
        "nlp=client\n",
        "text=\"The musical store receives tape requests from customers.\"\n",
        "output = nlp.semgrex(text, pattern='{tag: VBZ}', filter=False)\n",
        "print(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'sentences': [True]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJ49_RwUU-H5",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yb3lwcFKOtQr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "801fed38-b539-4c9d-9196-d3572e91d236"
      },
      "source": [
        "text = \"Barack Obama was not born in Hawaii. He was born in Newyork.\"\n",
        "#text=\"The musical store receives tape requests from customers.\"\n",
        "#text=\"System verifies login information for candidate\"\n",
        "# set up the client\n",
        "#with CoreNLPClient(annotators=['tokenize','ssplit','pos','lemma','ner', 'depparse'], timeout=60000, memory='16G') as client:\n",
        "# submit the request to the server\n",
        "ann = client.annotate(text)\n",
        "# You can access annotations using ann.\n",
        "sentence = ann.sentence[0]\n",
        "\n",
        "# The corenlp.to_text function is a helper function that\n",
        "# reconstructs a sentence from tokens.\n",
        "\n",
        "#print(ann.text)\n",
        "# You can access any property within a sentence.\n",
        "for sentence in ann.sentence:\n",
        "  for token in sentence.token:\n",
        "    print(token.word,token.pos,token.ner)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Barack NNP PERSON\n",
            "Obama NNP PERSON\n",
            "was VBD O\n",
            "not RB O\n",
            "born VBN O\n",
            "in IN O\n",
            "Hawaii NNP STATE_OR_PROVINCE\n",
            ". . O\n",
            "He PRP O\n",
            "was VBD O\n",
            "born VBN O\n",
            "in IN O\n",
            "Newyork NNP LOCATION\n",
            ". . O\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4x5RCAmRf-W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "881db91f-d4a9-42d9-c53a-70ca0ee9f3fb"
      },
      "source": [
        "#def depparse(text):\n",
        "text=\"Chris Manning is a nice person. Chris wrote a simple sentence. He also gives oranges to people.\"\n",
        "\n",
        "parsed=\"\"\n",
        "nlp=client\n",
        "\n",
        "output = nlp.annotate(text, properties={\n",
        "  'annotators': 'depparse',\n",
        "  'outputFormat': 'json'\n",
        "  })\n",
        "for i in output['sentences']: # not sure if there can be multiple items here. If so, it just returns the first one currently.\n",
        "  print( [tuple((dep['dep'], dep['governorGloss'], dep['dependentGloss'])) for dep in i['basicDependencies']])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('ROOT', 'ROOT', 'person'), ('compound', 'Manning', 'Chris'), ('nsubj', 'person', 'Manning'), ('cop', 'person', 'is'), ('det', 'person', 'a'), ('amod', 'person', 'nice'), ('punct', 'person', '.')]\n",
            "[('ROOT', 'ROOT', 'wrote'), ('nsubj', 'wrote', 'Chris'), ('det', 'sentence', 'a'), ('amod', 'sentence', 'simple'), ('dobj', 'wrote', 'sentence'), ('punct', 'wrote', '.')]\n",
            "[('ROOT', 'ROOT', 'gives'), ('nsubj', 'gives', 'He'), ('advmod', 'gives', 'also'), ('dobj', 'gives', 'oranges'), ('case', 'people', 'to'), ('nmod', 'gives', 'people'), ('punct', 'gives', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlGE1rjg2oF3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re \n",
        "import string \n",
        "import nltk \n",
        "import spacy \n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import math \n",
        "from tqdm import tqdm \n",
        "\n",
        "from spacy.matcher import Matcher \n",
        "from spacy.tokens import Span \n",
        "from spacy import displacy \n",
        "\n",
        "pd.set_option('display.max_colwidth', 200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPYE_ros2vw3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmGUrx8VtZnq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#2. Split compund sentence\n",
        "def split_sentence(consent):\n",
        "  simsent=list()\n",
        "  sent=\"\"\n",
        "  d = nlp(consent)\n",
        "  for tok in d:\n",
        "    #print(tok.text)\n",
        "    if tok.text in ('AND','OR','and','or',';'):\n",
        "      simsent.append(sent)\n",
        "      print(sent)\n",
        "      sent=\"\"\n",
        "    else:\n",
        "      sent=sent+\" \"+tok.text\n",
        "  simsent.append(sent)\n",
        "  return simsent\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0sfwpR9vAcx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "fcfa3486-c589-41a4-e6fc-76b9088e8a0c"
      },
      "source": [
        "sent=split_sentence(\"He is good and he is old\")\n",
        "for s in sent:\n",
        "  print(s)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " He is good\n",
            " He is good\n",
            " he is old\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asLNJ31rWunA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp=client\n",
        "text=\"System verifies login information for candidate\"\n",
        "text = \"Chris wrote a simple sentence. he parsed with Stanford CoreNLP.\"\n",
        "output = nlp.annotate(text, properties={\"annotators\":\"tokenize,ssplit,pos,depparse,natlog,openie\",\n",
        "                            \"outputFormat\": \"json\",\n",
        "                             \"openie.triple.strict\":\"true\",\n",
        "                             \"openie.max_entailments_per_clause\":\"1\"})\n",
        "result = [output[\"sentences\"][0][\"openie\"] for item in output]\n",
        "for i in result:\n",
        "    #print(i)\n",
        "    for rel in i:\n",
        "        relationSent=rel['subject'],rel['relation'],rel['object']\n",
        "        #S=rel['subject']\n",
        "        #O=rel['object']\n",
        "        #A=rel['relation']\n",
        "        #print(S,A,O)\n",
        "        print(relationSent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1prLBFzyFVx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "e3c89cfe-280c-4c11-d9d9-64d278b1a6ad"
      },
      "source": [
        "text = \"Chris wrote a simple sentence that he parsed with Stanford CoreNLP.\"\n",
        "\n",
        "ann = client.annotate(text)\n",
        "\n",
        "# You can access annotations using ann.\n",
        "sentence = ann.sentence[0]\n",
        "# Likewise for tokens\n",
        "token = sentence.token[0]\n",
        "print(token.lemma)\n",
        "\n",
        "# Use tokensregex patterns to find who wrote a sentence.\n",
        "pattern = '([ner: PERSON]+) /wrote/ /an?/ []{0,3} /sentence|article/'\n",
        "matches = client.tokensregex(text, pattern)\n",
        "print(matches)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-30c5cb269a85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Chris wrote a simple sentence that he parsed with Stanford CoreNLP.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mann\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# You can access annotations using ann.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/stanfordnlp/server/client.py\u001b[0m in \u001b[0;36mannotate\u001b[0;34m(self, text, annotators, output_format, properties_key, properties, **kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m                 \u001b[0mrequest_properties\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'outputFormat'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCoreNLPClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFAULT_OUTPUT_FORMAT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;31m# make the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_properties\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrequest_properties\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"outputFormat\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"json\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/stanfordnlp/server/client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, buf, properties, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrequest\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \"\"\"\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/stanfordnlp/server/client.py\u001b[0m in \u001b[0;36mensure_alive\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCHECK_ALIVE_TIMEOUT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mPermanentlyFailedException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Timed out waiting for service to come alive.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhZqWE7u_WnR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use semgrex patterns to directly find who wrote what.\n",
        "pattern = '{word:wrote} >nsubj {}=subject >dobj {}=object'\n",
        "matches = client.semgrex(text, pattern)\n",
        "\n",
        "#print(matches[\"sentences\"][1][\"0\"][\"text\"])\n",
        "#print(matches[\"sentences\"][1][\"0\"][\"$subject\"][\"text\"] )\n",
        "#print(matches[\"sentences\"][1][\"0\"][\"$object\"][\"text\"] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePXcvArITt6y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "a61baf8e-dc18-4af3-d65a-910917f49f86"
      },
      "source": [
        "nlp=client\n",
        "text = 'Mark Robert is the founder of 3trucks. 3trucks was founded in 2010'\n",
        "\n",
        "output = nlp.annotate(text, properties={\n",
        "'annotators': 'tokenize,ssplit,pos,depparse,parse',\n",
        "\"timeout\": \"50000\",\n",
        "'outputFormat': 'json'\n",
        "\n",
        " })\n",
        "\n",
        "print(output['sentences'][0]['parse'])\n",
        "print('------------------------------')\n",
        "print(output['sentences'][1]['parse'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(ROOT\n",
            "  (S\n",
            "    (NP (NNP Mark) (NNP Robert))\n",
            "    (VP (VBZ is)\n",
            "      (NP\n",
            "        (NP (DT the) (NN founder))\n",
            "        (PP (IN of)\n",
            "          (NP (NNS 3trucks)))))\n",
            "    (. .)))\n",
            "------------------------------\n",
            "(ROOT\n",
            "  (S\n",
            "    (NP (NNS 3trucks))\n",
            "    (VP (VBD was)\n",
            "      (VP (VBN founded)\n",
            "        (PP (IN in)\n",
            "          (NP (CD 2010)))))))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Xw-urPBftBb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text=\"The musical store receives tape requests from customers. The musical store receives \\\\\n",
        "new tapes from the Main office. Musical store sends overdue notice to customers. \\\\\n",
        "Store assistant takes care of tape requests. Store assistant update the rental list. Store \\\\\n",
        "management submits the price changes. Store management submits new tapes. Store \\\\\n",
        "administration produces rental reports. Main office sends overdue notices for tapes. \\\\\n",
        "Customer request for a tape. Store assistant checks the availability of requested tape.\\\\\n",
        "Store assistant searches for the available tape. Store assistant searches for the rental \\\\\n",
        "price of available tape. Store assistant checks status of the tape to be returned by \\\\\n",
        "customer. Customer can borrow if there is no delay with return of other tapes. Store \\\\\n",
        "assistant records rental by updating the rental list. Store assistant asks customer for \\\\\n",
        "his address.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnnuvIGcg1u5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vU94qMoPlnGe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "outputId": "4063b926-9742-4f8f-baed-f35d00738865"
      },
      "source": [
        "output = client.annotate(text, properties={'annotators':'dcoref','outputFormat':'json','ner.useSUTime':'false'})\n",
        "\n",
        "coreferences = output['corefs']\n",
        "print(coreferences)\n",
        "print(document.hasCorefAnnotation)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ReadTimeout",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mtimeout\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    383\u001b[0m                     \u001b[0;31m# otherwise it looks like a programming error was the cause.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m                     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1345\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1346\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1347\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mtimeout\u001b[0m: timed out",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    637\u001b[0m             retries = retries.increment(method, url, error=e, _pool=self,\n\u001b[0;32m--> 638\u001b[0;31m                                         _stacktrace=sys.exc_info()[2])\n\u001b[0m\u001b[1;32m    639\u001b[0m             \u001b[0mretries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_method_retryable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_raise_timeout\u001b[0;34m(self, err, url, timeout_value)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketTimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mReadTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Read timed out. (read timeout=%s)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mReadTimeoutError\u001b[0m: HTTPConnectionPool(host='localhost', port=9001): Read timed out. (read timeout=20.0)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mReadTimeout\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-8b72242d4e61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'annotators'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'dcoref'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'outputFormat'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'json'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ner.useSUTime'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'false'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcoreferences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'corefs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoreferences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasCorefAnnotation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/stanfordnlp/server/client.py\u001b[0m in \u001b[0;36mannotate\u001b[0;34m(self, text, annotators, output_format, properties_key, properties, **kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m                 \u001b[0mrequest_properties\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'outputFormat'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCoreNLPClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFAULT_OUTPUT_FORMAT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;31m# make the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_properties\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrequest_properties\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"outputFormat\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"json\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/stanfordnlp/server/client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, buf, properties, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m                               \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'properties'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproperties\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                               \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'content-type'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mctype\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m                               timeout=(self.timeout*2)/1000, **kwargs)\n\u001b[0m\u001b[1;32m    330\u001b[0m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \"\"\"\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    531\u001b[0m         }\n\u001b[1;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    527\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReadTimeoutError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mReadTimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mReadTimeout\u001b[0m: HTTPConnectionPool(host='localhost', port=9001): Read timed out. (read timeout=20.0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHTiP_3oKLrh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Sample data\n",
        "sam1=\"User needed money for fees. He went to the ATM. He \\\n",
        "entered password into the machine. User put the \\\n",
        "money in his pocket.\"\n",
        "sam2=\"The Person walks over to the ATM. ATM asks\\\n",
        "password from the user. The user enter password \\\n",
        "into the machine.\"\n",
        "sample=\"Each lift has a set of buttons, one for each floor. These illuminate when pressed and cause the lift to visit the corresponding floor. The illumination is cancelled when the corresponding floor is visited by the lift.\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cdM85vsI_zh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "outputId": "6cc46572-5f54-4b7b-f4e3-60d2aa90d753"
      },
      "source": [
        "#from pycorenlp import StanfordCoreNLP\n",
        "#Solving coreferences\n",
        "nlp = client #StanfordCoreNLP('http://localhost:9000')\n",
        "\n",
        "\n",
        "def resolve(corenlp_output):\n",
        "    \"\"\" Transfer the word form of the antecedent to its associated pronominal anaphor(s) \"\"\"\n",
        "    for coref in corenlp_output['corefs']:\n",
        "        mentions = corenlp_output['corefs'][coref]\n",
        "        antecedent = mentions[0]  # the antecedent is the first mention in the coreference chain\n",
        "        for j in range(1, len(mentions)):\n",
        "            mention = mentions[j]\n",
        "            if mention['type'] == 'PRONOMINAL':\n",
        "                # get the attributes of the target mention in the corresponding sentence\n",
        "                target_sentence = mention['sentNum']\n",
        "                target_token = mention['startIndex'] - 1\n",
        "                # transfer the antecedent's word form to the appropriate token in the sentence\n",
        "                corenlp_output['sentences'][target_sentence - 1]['tokens'][target_token]['word'] = antecedent['text']\n",
        "\n",
        "\n",
        "def print_resolved(corenlp_output):\n",
        "    \"\"\" Print the \"resolved\" output \"\"\"\n",
        "    possessives = ['hers', 'his', 'their', 'theirs']\n",
        "    for sentence in corenlp_output['sentences']:\n",
        "        for token in sentence['tokens']:\n",
        "            output_word = token['word']\n",
        "            # check lemmas as well as tags for possessive pronouns in case of tagging errors\n",
        "            if token['lemma'] in possessives or token['pos'] == 'PRP$':\n",
        "                output_word += \"'s\"  # add the possessive morpheme\n",
        "            output_word += token['after']\n",
        "            print(output_word, end='')\n",
        "\n",
        "\n",
        "text =\"Tom and Jane are good friends. They are cool. He knows a lot of things and so does she. His car is red, but \" \\\n",
        "       \"hers is blue. It is older than hers. The big cat ate its dinner.\"\n",
        "\n",
        "output = nlp.annotate(text, properties= {'annotators':'dcoref','outputFormat':'json','ner.useSUTime':'false'})\n",
        "\n",
        "resolve(output)\n",
        "\n",
        "print('Original:', text)\n",
        "print('Resolved: ', end='')\n",
        "print_resolved(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ReadTimeout",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mtimeout\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    383\u001b[0m                     \u001b[0;31m# otherwise it looks like a programming error was the cause.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m                     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1345\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1346\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1347\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mtimeout\u001b[0m: timed out",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    637\u001b[0m             retries = retries.increment(method, url, error=e, _pool=self,\n\u001b[0;32m--> 638\u001b[0;31m                                         _stacktrace=sys.exc_info()[2])\n\u001b[0m\u001b[1;32m    639\u001b[0m             \u001b[0mretries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_method_retryable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_raise_timeout\u001b[0;34m(self, err, url, timeout_value)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketTimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mReadTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Read timed out. (read timeout=%s)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mReadTimeoutError\u001b[0m: HTTPConnectionPool(host='localhost', port=9001): Read timed out. (read timeout=20.0)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mReadTimeout\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-7609f3950e2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m\"Tom and Jane are good friends. They are cool. He knows a lot of things and so does she. His car is red, but \"\u001b[0m        \u001b[0;34m\"hers is blue. It is older than hers. The big cat ate its dinner.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'annotators'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'dcoref'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'outputFormat'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'json'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ner.useSUTime'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'false'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/stanfordnlp/server/client.py\u001b[0m in \u001b[0;36mannotate\u001b[0;34m(self, text, annotators, output_format, properties_key, properties, **kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m                 \u001b[0mrequest_properties\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'outputFormat'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCoreNLPClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFAULT_OUTPUT_FORMAT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;31m# make the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_properties\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrequest_properties\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"outputFormat\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"json\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/stanfordnlp/server/client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, buf, properties, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m                               \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'properties'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproperties\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                               \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'content-type'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mctype\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m                               timeout=(self.timeout*2)/1000, **kwargs)\n\u001b[0m\u001b[1;32m    330\u001b[0m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \"\"\"\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    531\u001b[0m         }\n\u001b[1;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    527\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReadTimeoutError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mReadTimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mReadTimeout\u001b[0m: HTTPConnectionPool(host='localhost', port=9001): Read timed out. (read timeout=20.0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW6TaYlzLz6K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "4da252ed-c283-4967-bda0-b7ecf8d0c596"
      },
      "source": [
        "#onverting complex sentence to simple\n",
        "import re\n",
        "\n",
        "text = \"\"\"\\\n",
        "Mr. Smith bought cheapsite.com or 1.5 million dollars, and i.e. he paid a lot for it. Did he mind? Adam Jones Jr. thinks he didn't. In any case, this isn't true... Well, with a probability of .9 it isn't.\n",
        "\"\"\"\n",
        "#sentences = re.split(r' *[\\.\\?!][\\'\"\\)\\]]* *', text)\n",
        "sentences = re.split(r'(and|or)', text)\n",
        "for stuff in sentences:\n",
        "        print(stuff)\n",
        "re.sub('(a|an|and|the|is)', \"\", text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mr. Smith bought cheapsite.com \n",
            "or\n",
            " 1.5 million dollars, \n",
            "and\n",
            " i.e. he paid a lot f\n",
            "or\n",
            " it. Did he mind? Adam Jones Jr. thinks he didn't. In any case, this isn't true... Well, with a probability of .9 it isn't.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Mr. Smith bought chepsite.com or 1.5 million dollrs, nd i.e. he pid  lot for it. Did he mind? Adm Jones Jr. thinks he didn't. In ny cse, th n't true... Well, with  probbility of .9 it n't.\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rV689BnySXTN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "f5d6f303-2211-4484-d06c-4c79acbdec6d"
      },
      "source": [
        "import nltk\n",
        "from nltk import Tree\n",
        "groucho_grammar = nltk.CFG.fromstring(\"\"\"\n",
        "S -> NP VP\n",
        "PP -> P NP\n",
        "NP -> Det N | Det N PP | 'I'\n",
        "VP -> V NP | VP PP\n",
        "Det -> 'an' | 'my'\n",
        "N -> 'elephant' | 'pajamas'\n",
        "V -> 'shot'\n",
        "P -> 'in'\n",
        "\"\"\")\n",
        "#from nltk import groucho_grammar\n",
        "\n",
        "sent = ['I', 'shot', 'an', 'elephant', 'in', 'my', 'pajamas']\n",
        "parser = nltk.ChartParser(groucho_grammar)\n",
        "for tree in parser.parse(sent):\n",
        "  print(tree)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S\n",
            "  (NP I)\n",
            "  (VP\n",
            "    (VP (V shot) (NP (Det an) (N elephant)))\n",
            "    (PP (P in) (NP (Det my) (N pajamas)))))\n",
            "(S\n",
            "  (NP I)\n",
            "  (VP\n",
            "    (V shot)\n",
            "    (NP (Det an) (N elephant) (PP (P in) (NP (Det my) (N pajamas))))))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGolyslwdMD_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d5b627cb-363c-4dd3-bcdd-69779648aaad"
      },
      "source": [
        "#from stanfordnlp.server import CoreNLPClient\n",
        "from nltk import tokenize\n",
        "#nltk.download('punkt')\n",
        "client = client\n",
        "def pronoun_resolution(text):\n",
        "\n",
        "    ann = client.annotate(text)\n",
        "    modified_text = tokenize.sent_tokenize(text)\n",
        "\n",
        "    for coref in ann.corefChain:\n",
        "\n",
        "        antecedent = []\n",
        "        for mention in coref.mention:\n",
        "            phrase = []\n",
        "            for i in range(mention.beginIndex, mention.endIndex):\n",
        "                phrase.append(ann.sentence[mention.sentenceIndex].token[i].word)\n",
        "            if antecedent == []:\n",
        "                antecedent = ' '.join(word for word in phrase)\n",
        "            else:\n",
        "                anaphor = ' '.join(word for word in phrase)\n",
        "                modified_text[mention.sentenceIndex] = modified_text[mention.sentenceIndex].replace(anaphor, antecedent)\n",
        "\n",
        "    modified_text = ' '.join(modified_text)\n",
        "\n",
        "    return modified_text\n",
        "\n",
        "text = 'Tom is a smart boy. He knows a lot of things.'\n",
        "print(pronoun_resolution(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tom is a smart boy. He knows a lot of things.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGdm3THnlHPp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "outputId": "5410527b-1c1c-4b98-bf72-4aeb7e54eb1a"
      },
      "source": [
        "from stanfordnlp.server import CoreNLPClient\n",
        "\n",
        "text = 'Barack was born in Hawaii. His wife Michelle was born in Milan. He says that she is very smart.'\n",
        "print(f\"Input text: {text}\")\n",
        "\n",
        "# set up the client\n",
        "client = CoreNLPClient(properties={'annotators': 'coref', 'coref.algorithm' : 'statistical'}, timeout=60000, memory='16G')\n",
        "\n",
        "# submit the request to the server\n",
        "ann = client.annotate(text)    \n",
        "\n",
        "mychains = list()\n",
        "print(ann.hasCorefAnnotation)\n",
        "chains = ann.corefChain\n",
        "print(chains)\n",
        "for chain in chains:\n",
        "    mychain = list()\n",
        "    # Loop through every mention of this chain\n",
        "    for mention in chain.mention:\n",
        "        # Get the sentence in which this mention is located, and get the words which are part of this mention\n",
        "        # (we can have more than one word, for example, a mention can be a pronoun like \"he\", but also a compound noun like \"His wife Michelle\")\n",
        "        words_list = ann.sentence[mention.sentenceIndex].token[mention.beginIndex:mention.endIndex]\n",
        "        #build a string out of the words of this mention\n",
        "        ment_word = ' '.join([x.word for x in words_list])\n",
        "        mychain.append(ment_word)\n",
        "    mychains.append(mychain)\n",
        "\n",
        "for chain in mychains:\n",
        "    print(' <-> '.join(chain))\n",
        "#edited Jul 19 at 10:48\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input text: Barack was born in Hawaii. His wife Michelle was born in Milan. He says that she is very smart.\n",
            "Starting server with command: java -Xmx16G -cp ./corenlp/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-9ae256c370644b19.props -preload coref\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "PermanentlyFailedException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPermanentlyFailedException\u001b[0m                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-0d2d548fbd24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# submit the request to the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mann\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mmychains\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/stanfordnlp/server/client.py\u001b[0m in \u001b[0;36mannotate\u001b[0;34m(self, text, annotators, output_format, properties_key, properties, **kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m                 \u001b[0mrequest_properties\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'outputFormat'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCoreNLPClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFAULT_OUTPUT_FORMAT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;31m# make the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_properties\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrequest_properties\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"outputFormat\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"json\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/stanfordnlp/server/client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, buf, properties, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrequest\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \"\"\"\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/stanfordnlp/server/client.py\u001b[0m in \u001b[0;36mensure_alive\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mPermanentlyFailedException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Timed out waiting for service to come alive.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# At this point we are guaranteed that the service is alive.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPermanentlyFailedException\u001b[0m: Timed out waiting for service to come alive."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjTmPK9LqDz3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "outputId": "64d8f296-23ef-4107-e475-5180da892d65"
      },
      "source": [
        "\n",
        "\n",
        "from nltk.parse.stanford import StanfordDependencyParser\n",
        "sdp = StanfordDependencyParser(path_to_jar='E:/stanford/stanford-parser-full-2015-04-20/stanford-parser.jar',\n",
        "                               path_to_models_jar='E:/stanford/stanford-parser-full-2015-04-20/stanford-parser-3.5.2-models.jar')    \n",
        "\n",
        "result = list(sdp.raw_parse(sentence))  \n",
        "\n",
        "# print the dependency tree\n",
        "dep_tree = [parse.tree() for parse in result][0]\n",
        "print(dep_tree)\n",
        "\n",
        "# visualize raw dependency tree\n",
        "from IPython.display import display\n",
        "display(dep_tree)\n",
        "\n",
        "# visualize annotated dependency tree (needs graphviz)\n",
        "from graphviz import Source\n",
        "dep_tree_dot_repr = [parse for parse in result][0].to_dot()\n",
        "source = Source(dep_tree_dot_repr, filename=\"dep_tree\", format=\"png\")\n",
        "source"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-a29bed055846>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstanford\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStanfordDependencyParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msdp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStanfordDependencyParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/parse/stanford.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_to_jar, path_to_models_jar, model_path, encoding, verbose, java_options, corenlp_options)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_regex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             ),\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         )\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/__init__.py\u001b[0m in \u001b[0;36mfind_jar_iter\u001b[0;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[1;32m    714\u001b[0m                     (name_pattern, url))\n\u001b[1;32m    715\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'='\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m75\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\n%s\\n%s\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m def find_jar(name_pattern, path_to_jar=None, env_vars=(),\n",
            "\u001b[0;31mLookupError\u001b[0m: \n\n===========================================================================\n  NLTK was unable to find stanford-parser\\.jar! Set the CLASSPATH\n  environment variable.\n\n  For more information, on stanford-parser\\.jar, see:\n    <https://nlp.stanford.edu/software/lex-parser.shtml>\n==========================================================================="
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YaeJcS9YMzP",
        "colab_type": "text"
      },
      "source": [
        "https://medium.com/analytics-vidhya/introduction-to-information-extraction-using-python-and-spacy-858f5d6416ca"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZEqdRxyQFQt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re \n",
        "import string \n",
        "import nltk \n",
        "import spacy \n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import math \n",
        "from tqdm import tqdm \n",
        "\n",
        "from spacy.matcher import Matcher \n",
        "from spacy.tokens import Span \n",
        "from spacy import displacy \n",
        "\n",
        "pd.set_option('display.max_colwidth', 200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVw6l07EQgcc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EK1ejEQQq0r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sample text \n",
        "text = \"GDP in developing countries such as Vietnam will continue growing at a high rate.\" \n",
        "\n",
        "# create a spaCy object \n",
        "doc = nlp(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsnsB5_RQyGi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "215b0ed6-c9c7-46e4-854d-1c9f2163638a"
      },
      "source": [
        "# print token, dependency, POS tag \n",
        "for tok in doc: \n",
        "  print(tok.text, \"-->\",tok.dep_,\"-->\", tok.pos_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GDP --> nsubj --> NOUN\n",
            "in --> prep --> ADP\n",
            "developing --> amod --> VERB\n",
            "countries --> pobj --> NOUN\n",
            "such --> amod --> ADJ\n",
            "as --> prep --> ADP\n",
            "Vietnam --> pobj --> PROPN\n",
            "will --> aux --> VERB\n",
            "continue --> ROOT --> VERB\n",
            "growing --> xcomp --> VERB\n",
            "at --> prep --> ADP\n",
            "a --> det --> DET\n",
            "high --> amod --> ADJ\n",
            "rate --> pobj --> NOUN\n",
            ". --> punct --> PUNCT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tlccS3dQ6kt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define the pattern \n",
        "pattern = [{'POS':'NOUN'}, \n",
        "           {'LOWER': 'such'}, \n",
        "           {'LOWER': 'as'}, \n",
        "           {'POS': 'PROPN'} ]#proper noun]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Llcj3_SRa89",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a8307f9c-c08d-44e5-fec9-0dfbe871990e"
      },
      "source": [
        "# Matcher class object \n",
        "matcher = Matcher(nlp.vocab) \n",
        "matcher.add(\"matching_1\", None, pattern) \n",
        "\n",
        "matches = matcher(doc) \n",
        "span = doc[matches[0][1]:matches[0][2]] \n",
        "\n",
        "print(span.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "countries such as Vietnam\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6dmAK8NRuGZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "89974922-b1e7-4397-a1aa-39b49ec881b2"
      },
      "source": [
        "# Matcher class object\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "#define the pattern\n",
        "pattern = [{'DEP':'amod', 'OP':\"?\"}, # adjectival modifier\n",
        "           {'POS':'NOUN'},\n",
        "           {'LOWER': 'such'},\n",
        "           {'LOWER': 'as'},\n",
        "           {'POS': 'PROPN'}]\n",
        "\n",
        "matcher.add(\"matching_1\", None, pattern)\n",
        "matches = matcher(doc)\n",
        "\n",
        "span = doc[matches[0][1]:matches[0][2]]\n",
        "print(span.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "developing countries such as Vietnam\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Jh8K17FR9bQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "4499c719-1697-466f-8ab9-4a97a11729da"
      },
      "source": [
        "######1. find subj obj verb\n",
        "text=\"user selects the company for which he wants to apply.\"\n",
        "#text=\"Here is how you can keep your car and other vehicles clean.\"\n",
        "doc = nlp(text) \n",
        "\n",
        "# print dependency tags and POS tags\n",
        "for tok in doc: \n",
        "  #print(tok.text, \"-->\",tok.dep_, \"-->\",tok.pos_)\n",
        "  print(tok.text,\"/\",tok.pos_,\"/\",tok.dep_,\"/\",tok.tag_,\"/\",tok.orth_)\n",
        "for tok in doc:\n",
        "  if tok.tag_ in ('NN','NP') and tok.dep_ in ('nsubj'):\n",
        "    sub=tok.text\n",
        "\n",
        "for tok in doc:\n",
        "  if tok.tag_ in ('NN','NP') and tok.dep_ in ('dobj'):\n",
        "    obj=tok.text\n",
        "\n",
        "for tok in doc:\n",
        "  if tok.tag_ in ('VBZ') and tok.dep_ in ('ROOT'):\n",
        "    action=tok.text\n",
        "print(sub,obj,action)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "user / NOUN / nsubj / NN / user\n",
            "selects / VERB / ROOT / VBZ / selects\n",
            "the / DET / det / DT / the\n",
            "company / NOUN / dobj / NN / company\n",
            "for / ADP / prep / IN / for\n",
            "which / DET / pobj / WDT / which\n",
            "he / PRON / nsubj / PRP / he\n",
            "wants / VERB / relcl / VBZ / wants\n",
            "to / PART / aux / TO / to\n",
            "apply / VERB / xcomp / VB / apply\n",
            ". / PUNCT / punct / . / .\n",
            "user company selects\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZJZNcriIrgT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0gj9HtIonN2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "39691a11-250d-4fac-8462-0d9e5b74fa64"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "parsed_text = nlp(u\"I thought it was the complete set\")\n",
        "direct_object=\"\"\n",
        "indirect_object=\"\"\n",
        "#get token dependencies\n",
        "for text in parsed_text:\n",
        "    #subject would be\n",
        "    if text.dep_ == \"nsubj\":\n",
        "        subject = text.orth_\n",
        "    #iobj for indirect object\n",
        "    if text.dep_ == \"iobj\":\n",
        "        indirect_object = text.orth_\n",
        "    #dobj for direct object\n",
        "    if text.dep_ == \"dobj\":\n",
        "        direct_object = text.orth_\n",
        "\n",
        "print(subject)\n",
        "print(direct_object)\n",
        "print(indirect_object)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "it\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujIV-113pUas",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "62b6ea74-8566-495e-ba4b-d950f5c29930"
      },
      "source": [
        "doc = nlp(\"I thought it was the complete set\")\n",
        "for nc in doc.noun_chunks:\n",
        "    print(nc.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I\n",
            "it\n",
            "the complete set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-P1WrftKp2Vt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "87b476a9-1409-4565-f782-3c3ee6059cf3"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "doc = nlp(u'KEEP CALM because TOGETHER We Rock !')\n",
        "for word in doc:\n",
        "    print(word.text, word.lemma, word.lemma_, word.tag, word.tag_, word.pos, word.pos_)\n",
        "    print(word.orth_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KEEP 10409167902684988128 KEEP 15794550382381185553 NNP 96 PROPN\n",
            "KEEP\n",
            "CALM 18347787977573315042 CALM 15794550382381185553 NNP 96 PROPN\n",
            "CALM\n",
            "because 16950148841647037698 because 1292078113972184607 IN 85 ADP\n",
            "because\n",
            "TOGETHER 13495226875806840535 TOGETHER 15794550382381185553 NNP 96 PROPN\n",
            "TOGETHER\n",
            "We 561228191312463089 -PRON- 13656873538139661788 PRP 95 PRON\n",
            "We\n",
            "Rock 3237817430745561104 Rock 15794550382381185553 NNP 96 PROPN\n",
            "Rock\n",
            "! 17494803046312582752 ! 12646065887601541794 . 97 PUNCT\n",
            "!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOj6YlaXSzNT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f1fd7196-bea3-49e1-cddc-7003abf48a45"
      },
      "source": [
        "# Matcher class object \n",
        "matcher = Matcher(nlp.vocab) \n",
        "\n",
        "#define the pattern \n",
        "pattern = [{'DEP':'amod', 'OP':\"?\"}, \n",
        "           {'POS':'NOUN'}, \n",
        "           {'LOWER': 'and', 'OP':\"?\"}, \n",
        "           {'LOWER': 'or', 'OP':\"?\"}, \n",
        "           {'LOWER': 'other'}, \n",
        "           {'POS': 'NOUN'}] \n",
        "           \n",
        "matcher.add(\"matching_1\", None, pattern) \n",
        "\n",
        "matches = matcher(doc) \n",
        "span = doc[matches[0][1]:matches[0][2]] \n",
        "print(span.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "car and other vehicles\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m96F6NcVTKys",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "28d414bd-4cc9-46f3-9f4e-f63124cece78"
      },
      "source": [
        "# replaced 'and' with 'or' \n",
        "doc = nlp(\"Here is how you can keep your car or other vehicles clean.\")\n",
        "# Matcher class object \n",
        "matcher = Matcher(nlp.vocab) \n",
        "\n",
        "#define the pattern \n",
        "pattern = [{'DEP':'amod', 'OP':\"?\"}, \n",
        "           {'POS':'NOUN'}, \n",
        "           {'LOWER': 'and', 'OP':\"?\"}, \n",
        "           {'LOWER': 'or', 'OP':\"?\"}, \n",
        "           {'LOWER': 'other'}, \n",
        "           {'POS': 'NOUN'}] \n",
        "           \n",
        "matcher.add(\"matching_1\", None, pattern) \n",
        "\n",
        "matches = matcher(doc) \n",
        "span = doc[matches[0][1]:matches[0][2]] \n",
        "print(span.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "car or other vehicles\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_nOIFzJTgnM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7289939e-d358-4328-9bea-c59b2cb00db9"
      },
      "source": [
        "doc = nlp(\"A healthy eating pattern including fruits, including whole fruits.\") \n",
        "\n",
        "# Matcher class object \n",
        "matcher = Matcher(nlp.vocab) \n",
        "\n",
        "#define the pattern \n",
        "pattern = [{'DEP':'nummod','OP':\"?\"}, # numeric modifier \n",
        "           {'DEP':'amod','OP':\"?\"}, # adjectival modifier \n",
        "           {'POS':'NOUN'}, \n",
        "           {'IS_PUNCT': True}, \n",
        "           {'LOWER': 'including'}, \n",
        "           {'DEP':'nummod','OP':\"?\"}, \n",
        "           {'DEP':'amod','OP':\"?\"}, \n",
        "           {'POS':'NOUN'}] \n",
        "                               \n",
        "matcher.add(\"matching_1\", None, pattern) \n",
        "\n",
        "matches = matcher(doc) \n",
        "span = doc[matches[0][1]:matches[0][2]] \n",
        "print(span.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fruits, including whole fruits\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knNTtuiOUk6O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "61252694-5225-439b-ed29-641322aa7dce"
      },
      "source": [
        "doc = nlp(\"A healthy eating pattern includes fruits, especially whole fruits.\") \n",
        "\n",
        "for tok in doc: \n",
        "  print(tok.text, tok.dep_, tok.pos_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A det DET\n",
            "healthy amod ADJ\n",
            "eating compound NOUN\n",
            "pattern nsubj NOUN\n",
            "includes ROOT VERB\n",
            "fruits dobj NOUN\n",
            ", punct PUNCT\n",
            "especially advmod ADV\n",
            "whole amod ADJ\n",
            "fruits appos NOUN\n",
            ". punct PUNCT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-u0bE2SU5oj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "692c6e56-773b-469f-a801-bce2687ba3a3"
      },
      "source": [
        "# Matcher class object \n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "#define the pattern \n",
        "pattern = [{'DEP':'nummod','OP':\"?\"}, \n",
        "           {'DEP':'amod','OP':\"?\"}, \n",
        "           {'POS':'NOUN'}, \n",
        "           {'IS_PUNCT':True}, \n",
        "           {'LOWER': 'especially'}, \n",
        "           {'DEP':'nummod','OP':\"?\"}, \n",
        "           {'DEP':'amod','OP':\"?\"}, \n",
        "           {'POS':'NOUN'}] \n",
        "           \n",
        "matcher.add(\"matching_1\", None, pattern) \n",
        "\n",
        "matches = matcher(doc) \n",
        "span = doc[matches[0][1]:matches[0][2]] \n",
        "print(span.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fruits, especially whole fruits\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWeevWsZVK30",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "outputId": "a077c686-3801-40fe-8745-3f3e2dd6d5e5"
      },
      "source": [
        "#dependency\n",
        "text = \"Tableau was recently acquired by Salesforce.\" \n",
        "\n",
        "# Plot the dependency graph \n",
        "doc = nlp(text) \n",
        "displacy.render(doc, style='dep',jupyter=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"dc5597509382435bb211d8461dff7754-0\" class=\"displacy\" width=\"1100\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Tableau</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">was</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">recently</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADV</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">acquired</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">by</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">Salesforce.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-dc5597509382435bb211d8461dff7754-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,2.0 575.0,2.0 575.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-dc5597509382435bb211d8461dff7754-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubjpass</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-dc5597509382435bb211d8461dff7754-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,89.5 570.0,89.5 570.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-dc5597509382435bb211d8461dff7754-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">auxpass</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-dc5597509382435bb211d8461dff7754-0-2\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-dc5597509382435bb211d8461dff7754-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-dc5597509382435bb211d8461dff7754-0-3\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-dc5597509382435bb211d8461dff7754-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">agent</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M740.0,266.5 L748.0,254.5 732.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-dc5597509382435bb211d8461dff7754-0-4\" stroke-width=\"2px\" d=\"M770,264.5 C770,177.0 915.0,177.0 915.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-dc5597509382435bb211d8461dff7754-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M915.0,266.5 L923.0,254.5 907.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odGtNZadVi-A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "4d77dda6-d05c-4a82-b9b8-d32f02c57b57"
      },
      "source": [
        "text='“Careem, a ride-hailing major in the middle east, was acquired by Uber.”'\n",
        "# Plot the dependency graph \n",
        "doc = nlp(text) \n",
        "displacy.render(doc, style='dep',jupyter=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"1afc40a93b4342038665941dc0807c6e-0\" class=\"displacy\" width=\"2500\" height=\"574.5\" direction=\"ltr\" style=\"max-width: none; height: 574.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">“</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">Careem,</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">a</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">ride-</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">hailing</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">major</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">in</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">the</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">middle</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">east,</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">was</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">acquired</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2150\">by</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2150\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2325\">Uber.”</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2325\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-1afc40a93b4342038665941dc0807c6e-0-0\" stroke-width=\"2px\" d=\"M70,439.5 C70,352.0 205.0,352.0 205.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-1afc40a93b4342038665941dc0807c6e-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,441.5 L62,429.5 78,429.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-1afc40a93b4342038665941dc0807c6e-0-1\" stroke-width=\"2px\" d=\"M245,439.5 C245,2.0 1975.0,2.0 1975.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-1afc40a93b4342038665941dc0807c6e-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubjpass</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M245,441.5 L237,429.5 253,429.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-1afc40a93b4342038665941dc0807c6e-0-2\" stroke-width=\"2px\" d=\"M420,439.5 C420,177.0 915.0,177.0 915.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-1afc40a93b4342038665941dc0807c6e-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M420,441.5 L412,429.5 428,429.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-1afc40a93b4342038665941dc0807c6e-0-3\" stroke-width=\"2px\" d=\"M595,439.5 C595,352.0 730.0,352.0 730.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-1afc40a93b4342038665941dc0807c6e-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">npadvmod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M595,441.5 L587,429.5 603,429.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-1afc40a93b4342038665941dc0807c6e-0-4\" stroke-width=\"2px\" d=\"M770,439.5 C770,352.0 905.0,352.0 905.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-1afc40a93b4342038665941dc0807c6e-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M770,441.5 L762,429.5 778,429.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-1afc40a93b4342038665941dc0807c6e-0-5\" stroke-width=\"2px\" d=\"M245,439.5 C245,89.5 920.0,89.5 920.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-1afc40a93b4342038665941dc0807c6e-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">appos</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M920.0,441.5 L928.0,429.5 912.0,429.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-1afc40a93b4342038665941dc0807c6e-0-6\" stroke-width=\"2px\" d=\"M945,439.5 C945,352.0 1080.0,352.0 1080.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-1afc40a93b4342038665941dc0807c6e-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1080.0,441.5 L1088.0,429.5 1072.0,429.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-1afc40a93b4342038665941dc0807c6e-0-7\" stroke-width=\"2px\" d=\"M1295,439.5 C1295,264.5 1610.0,264.5 1610.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-1afc40a93b4342038665941dc0807c6e-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1295,441.5 L1287,429.5 1303,429.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-1afc40a93b4342038665941dc0807c6e-0-8\" stroke-width=\"2px\" d=\"M1470,439.5 C1470,352.0 1605.0,352.0 1605.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-1afc40a93b4342038665941dc0807c6e-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1470,441.5 L1462,429.5 1478,429.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-1afc40a93b4342038665941dc0807c6e-0-9\" stroke-width=\"2px\" d=\"M1120,439.5 C1120,177.0 1615.0,177.0 1615.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-1afc40a93b4342038665941dc0807c6e-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1615.0,441.5 L1623.0,429.5 1607.0,429.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-1afc40a93b4342038665941dc0807c6e-0-10\" stroke-width=\"2px\" d=\"M1820,439.5 C1820,352.0 1955.0,352.0 1955.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-1afc40a93b4342038665941dc0807c6e-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">auxpass</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1820,441.5 L1812,429.5 1828,429.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-1afc40a93b4342038665941dc0807c6e-0-11\" stroke-width=\"2px\" d=\"M1995,439.5 C1995,352.0 2130.0,352.0 2130.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-1afc40a93b4342038665941dc0807c6e-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">agent</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M2130.0,441.5 L2138.0,429.5 2122.0,429.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BURtK150V9iY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "2bb4315f-ea49-49b7-b048-f23fd22f465c"
      },
      "source": [
        "#Subtree matching\n",
        "text = \"Tableau was recently acquired by Salesforce.\" \n",
        "doc = nlp(text) \n",
        "\n",
        "for tok in doc: \n",
        "  print(tok.text,\"-->\",tok.dep_,\"-->\",tok.pos_)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tableau --> nsubjpass --> PROPN\n",
            "was --> auxpass --> VERB\n",
            "recently --> advmod --> ADV\n",
            "acquired --> ROOT --> VERB\n",
            "by --> agent --> ADP\n",
            "Salesforce --> pobj --> PROPN\n",
            ". --> punct --> PUNCT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6EhVeE0WJqg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def subtree_matcher(doc): \n",
        "  x = '' \n",
        "  y = '' \n",
        "  \n",
        "  # iterate through all the tokens in the input sentence \n",
        "  for i,tok in enumerate(doc): \n",
        "    # extract subject \n",
        "    if tok.dep_.find(\"subjpass\") == True: \n",
        "      y = tok.text \n",
        "      \n",
        "    # extract object \n",
        "    if tok.dep_.endswith(\"obj\") == True: \n",
        "      x = tok.text \n",
        "      \n",
        "  return x,y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8vi58WLWeSg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "279e5111-5071-44ed-f80e-84c53e79cb9d"
      },
      "source": [
        "subtree_matcher(doc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Salesforce', 'Tableau')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFa1e1HQWkg7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ae4f1e87-6218-474c-edc1-181066dd189a"
      },
      "source": [
        "text_2 = \"Careem, a ride hailing major in middle east, was acquired by Uber.\" \n",
        "\n",
        "doc_2 = nlp(text_2) \n",
        "subtree_matcher(doc_2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Uber', 'Careem')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt4bT91JW8yt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c08020b8-dc98-420b-e8d5-a31a6bda20cb"
      },
      "source": [
        "#Using active sentence instead of passiv\n",
        "text_3 = \"Salesforce recently acquired Tableau.\" \n",
        "doc_3 = nlp(text_3) \n",
        "subtree_matcher(doc_3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Tableau', '')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mn0nqgxXYrr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Sub tree matching for both active and passive sentences\n",
        "def new_subtree_matcher(doc):\n",
        "  subjpass = 0\n",
        "\n",
        "  for i,tok in enumerate(doc):\n",
        "    # find dependency tag that contains the text \"subjpass\"    \n",
        "    if tok.dep_.find(\"subjpass\") == True:\n",
        "      subjpass = 1\n",
        "\n",
        "  x = ''\n",
        "  y = ''\n",
        "\n",
        "  # if subjpass == 1 then sentence is passive\n",
        "  if subjpass == 1:\n",
        "    for i,tok in enumerate(doc):\n",
        "      if tok.dep_.find(\"subjpass\") == True:\n",
        "        y = tok.text\n",
        "\n",
        "      if tok.dep_.endswith(\"obj\") == True:\n",
        "        x = tok.text\n",
        "  \n",
        "  # if subjpass == 0 then sentence is not passive\n",
        "  else:\n",
        "    for i,tok in enumerate(doc):\n",
        "      if tok.dep_.endswith(\"subj\") == True:\n",
        "        x = tok.text\n",
        "\n",
        "      if tok.dep_.endswith(\"obj\") == True:\n",
        "        y = tok.text\n",
        "\n",
        "  return x,y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQWhr3WBXtV6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9c44fb58-2ac3-4fe4-d868-66a74660a78d"
      },
      "source": [
        "new_subtree_matcher(doc_3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Salesforce', 'Tableau')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gwMgYLKX1yr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c749bc7e-fecd-4617-8a04-8a3a1072a196"
      },
      "source": [
        "new_subtree_matcher(nlp(\"Tableau was recently acquired by Salesforce.\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Salesforce', 'Tableau')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjyJBQq6X9Hh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "3e9f4d20-9f55-44a9-e619-765eb7760f64"
      },
      "source": [
        "text=\"System verifies login information of candidate\"\n",
        "doc=nlp(text)\n",
        "# print dependency tags and POS tags\n",
        "for tok in doc: \n",
        "  print(tok.text, \"-->\",tok.dep_, \"-->\",tok.pos_)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "System --> compound --> NOUN\n",
            "verifies --> nsubj --> NOUN\n",
            "login --> ROOT --> VERB\n",
            "information --> dobj --> NOUN\n",
            "of --> prep --> ADP\n",
            "candidate --> pobj --> NOUN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BS2xg3sz6CP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "cb6fc6dc-2c85-4edd-abf8-45f979397c37"
      },
      "source": [
        "doc = nlp(\n",
        "        \"displaCy uses CSS and JavaScript to show you how computers \"\n",
        "        \"understand language\"\n",
        "    )\n",
        "\n",
        "# The easiest way is to find the head of the subtree you want, and then use\n",
        "# the `.subtree`, `.children`, `.lefts` and `.rights` iterators. `.subtree`\n",
        "# is the one that does what you're asking for most directly:\n",
        "for word in doc:\n",
        "    if word.dep_ in (\"xcomp\", \"ccomp\"):\n",
        "        print(\"\".join(w.text_with_ws for w in word.subtree))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "to show you how computers understand language\n",
            "how computers understand language\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jh8v8LJu0dML",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "72acb958-b828-430f-8f47-fa6297ceab05"
      },
      "source": [
        "# It'd probably be better for `word.subtree` to return a `Span` object\n",
        "# instead of a generator over the tokens. If you want the `Span` you can\n",
        "# get it via the `.right_edge` and `.left_edge` properties. The `Span`\n",
        "# object is nice because you can easily get a vector, merge it, etc.\n",
        "for word in doc:\n",
        "    if word.dep_ in (\"xcomp\", \"ccomp\"):\n",
        "        subtree_span = doc[word.left_edge.i : word.right_edge.i + 1]\n",
        "        print(subtree_span.text, \"|\", subtree_span.root.text)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "to show you how computers understand language | show\n",
            "how computers understand language | understand\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGNanRmA3f0b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1f3a6252-3117-4aa5-fb4f-dbb57c8869d0"
      },
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "matcher = Matcher(nlp.vocab)\n",
        "# Add match ID \"HelloWorld\" with no callback and one pattern\n",
        "pattern = [{\"LOWER\": \"hello\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"world\"}]\n",
        "matcher.add(\"HelloWorld\", None, pattern)\n",
        "\n",
        "doc = nlp(\"Hello, world! Hello world!\")\n",
        "matches = matcher(doc)\n",
        "for match_id, start, end in matches:\n",
        "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
        "    span = doc[start:end]  # The matched span\n",
        "    print(match_id, string_id, start, end, span.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15578876784678163569 HelloWorld 0 3 Hello, world\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-2hNhnh3msj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "matcher.add(\"HelloWorld\", None,\n",
        "            [{\"LOWER\": \"hello\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"world\"}],\n",
        "            [{\"LOWER\": \"hello\"}, {\"LOWER\": \"world\"}])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jtzrW6x40lT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "cabad0b5-245e-4938-9852-e5da4fe40506"
      },
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "matcher = Matcher(nlp.vocab)\n",
        "matched_sents = []  # Collect data of matched sentences to be visualized\n",
        "\n",
        "def collect_sents(matcher, doc, i, matches):\n",
        "    match_id, start, end = matches[i]\n",
        "    span = doc[start:end]  # Matched span\n",
        "    sent = span.sent  # Sentence containing matched span\n",
        "    # Append mock entity for match in displaCy style to matched_sents\n",
        "    # get the match span by ofsetting the start and end of the span with the\n",
        "    # start and end of the sentence in the doc\n",
        "    match_ents = [{\n",
        "        \"start\": span.start_char - sent.start_char,\n",
        "        \"end\": span.end_char - sent.start_char,\n",
        "        \"label\": \"MATCH\",\n",
        "    }]\n",
        "    matched_sents.append({\"text\": sent.text, \"ents\": match_ents})\n",
        "\n",
        "pattern = [{\"LOWER\": \"facebook\"}, {\"LEMMA\": \"be\"}, {\"POS\": \"ADV\", \"OP\": \"*\"},\n",
        "           {\"POS\": \"ADJ\"}]\n",
        "matcher.add(\"FacebookIs\", collect_sents, pattern)  # add pattern\n",
        "doc = nlp(\"I'd say that Facebook is evil. – Facebook is pretty cool, right?\")\n",
        "matches = matcher(doc)\n",
        "\n",
        "# Serve visualization of sentences containing match with displaCy\n",
        "# set manual=True to make displaCy render straight from a dictionary\n",
        "# (if you're not running the code within a Jupyer environment, you can\n",
        "# use displacy.serve instead)\n",
        "displacy.render(matched_sents, style=\"ent\", manual=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I\\'d say that \\n<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\\n    Facebook is evil\\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MATCH</span>\\n</mark>\\n.</div>\\n\\n<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\\n<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\\n    Facebook is pretty cool\\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MATCH</span>\\n</mark>\\n, right?</div>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd56tVa37LQc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "561e6d86-3518-4ae0-debb-c36c4bc28a32"
      },
      "source": [
        "import spacy\n",
        "import networkx as nx\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(u'Convulsions that occur after DTaP are caused by a fever.')\n",
        "\n",
        "print('sentence:'.format(doc))\n",
        "\n",
        "# Load spacy's dependency tree into a networkx graph\n",
        "edges = []\n",
        "for token in doc:\n",
        "    for child in token.children:\n",
        "        edges.append(('{0}'.format(token.lower_),\n",
        "                      '{0}'.format(child.lower_)))\n",
        "graph = nx.Graph(edges)\n",
        "# Get the length and path\n",
        "entity1 = 'Convulsions'.lower()\n",
        "entity2 = 'fever'\n",
        "print(nx.shortest_path_length(graph, source=entity1, target=entity2))\n",
        "print(nx.shortest_path(graph, source=entity1, target=entity2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sentence:\n",
            "3\n",
            "['convulsions', 'caused', 'by', 'fever']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBSmMyxI9YuC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "outputId": "c39fe6b2-8564-419c-cf97-b8842e4df372"
      },
      "source": [
        "from nltk.parse.stanford import StanfordParser\n",
        "from nltk.tree import ParentedTree, Tree\n",
        " \n",
        "parser = StanfordParser()\n",
        " \n",
        "# Parse the example sentence\n",
        "sent = 'A rare black squirrel has become a regular visitor to a suburban garden'\n",
        "t = list(parser.raw_parse(sent))[0]\n",
        "t = ParentedTree.convert(t)\n",
        " \n",
        "t.pretty_print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-eab0a1bad847>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParentedTree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStanfordParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Parse the example sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/parse/stanford.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_to_jar, path_to_models_jar, model_path, encoding, verbose, java_options, corenlp_options)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_regex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             ),\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         )\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/__init__.py\u001b[0m in \u001b[0;36mfind_jar_iter\u001b[0;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[1;32m    714\u001b[0m                     (name_pattern, url))\n\u001b[1;32m    715\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'='\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m75\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\n%s\\n%s\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m def find_jar(name_pattern, path_to_jar=None, env_vars=(),\n",
            "\u001b[0;31mLookupError\u001b[0m: \n\n===========================================================================\n  NLTK was unable to find stanford-parser\\.jar! Set the CLASSPATH\n  environment variable.\n\n  For more information, on stanford-parser\\.jar, see:\n    <https://nlp.stanford.edu/software/lex-parser.shtml>\n==========================================================================="
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve029Znk99DJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        " \n",
        "os.environ['STANFORD_PARSER'] = 'stanford-parser'\n",
        "os.environ['STANFORD_MODELS'] = 'stanford-parser'\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44wrecUX-y4V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "382a0c90-427b-49ec-a689-635634210786"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQJ9V-1G9-o7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "5a7490a5-2cc8-4a3a-cd94-df1f10fd0894"
      },
      "source": [
        "!pip install StanfordParser"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement StanfordParser (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for StanfordParser\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2EFYXNL__YZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b4a19282-2660-44d6-b563-43aeba578086"
      },
      "source": [
        "# Update / Install NLTK\n",
        "!pip install -U nltk\n",
        "\n",
        "# Download the Stanford NLP tools\n",
        "!wget http://nlp.stanford.edu/software/stanford-ner-2015-04-20.zip\n",
        "!wget http://nlp.stanford.edu/software/stanford-postagger-full-2015-04-20.zip\n",
        "!wget http://nlp.stanford.edu/software/stanford-parser-full-2015-04-20.zip\n",
        "# Extract the zip file.\n",
        "!unzip stanford-ner-2015-04-20.zip \n",
        "!unzip stanford-parser-full-2015-04-20.zip \n",
        "!unzip stanford-postagger-full-2015-04-20.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-cp36-none-any.whl size=1449908 sha256=8a1c0249b807f0ceb12cf75972457d091bae1c006af92abb22bb9c763edb091f\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
            "Successfully built nltk\n",
            "Installing collected packages: nltk\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.4.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nltk"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "--2019-11-26 04:37:59--  http://nlp.stanford.edu/software/stanford-ner-2015-04-20.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/software/stanford-ner-2015-04-20.zip [following]\n",
            "--2019-11-26 04:37:59--  https://nlp.stanford.edu/software/stanford-ner-2015-04-20.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 176961718 (169M) [application/zip]\n",
            "Saving to: ‘stanford-ner-2015-04-20.zip’\n",
            "\n",
            "stanford-ner-2015-0 100%[===================>] 168.76M  1.47MB/s    in 2m 13s  \n",
            "\n",
            "2019-11-26 04:40:12 (1.27 MB/s) - ‘stanford-ner-2015-04-20.zip’ saved [176961718/176961718]\n",
            "\n",
            "--2019-11-26 04:40:13--  http://nlp.stanford.edu/software/stanford-postagger-full-2015-04-20.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/software/stanford-postagger-full-2015-04-20.zip [following]\n",
            "--2019-11-26 04:40:13--  https://nlp.stanford.edu/software/stanford-postagger-full-2015-04-20.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 129905428 (124M) [application/zip]\n",
            "Saving to: ‘stanford-postagger-full-2015-04-20.zip’\n",
            "\n",
            "stanford-postagger- 100%[===================>] 123.89M   986KB/s    in 66s     \n",
            "\n",
            "2019-11-26 04:41:21 (1.86 MB/s) - ‘stanford-postagger-full-2015-04-20.zip’ saved [129905428/129905428]\n",
            "\n",
            "--2019-11-26 04:41:22--  http://nlp.stanford.edu/software/stanford-parser-full-2015-04-20.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/software/stanford-parser-full-2015-04-20.zip [following]\n",
            "--2019-11-26 04:41:22--  https://nlp.stanford.edu/software/stanford-parser-full-2015-04-20.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 308449252 (294M) [application/zip]\n",
            "Saving to: ‘stanford-parser-full-2015-04-20.zip’\n",
            "\n",
            "stanford-parser-ful 100%[===================>] 294.16M  1.59MB/s    in 3m 11s  \n",
            "\n",
            "2019-11-26 04:44:33 (1.54 MB/s) - ‘stanford-parser-full-2015-04-20.zip’ saved [308449252/308449252]\n",
            "\n",
            "Archive:  stanford-ner-2015-04-20.zip\n",
            "   creating: stanford-ner-2015-04-20/\n",
            "  inflating: stanford-ner-2015-04-20/README.txt  \n",
            "  inflating: stanford-ner-2015-04-20/ner-gui.bat  \n",
            "  inflating: stanford-ner-2015-04-20/build.xml  \n",
            "  inflating: stanford-ner-2015-04-20/stanford-ner-3.5.2.jar  \n",
            "  inflating: stanford-ner-2015-04-20/stanford-ner.jar  \n",
            "  inflating: stanford-ner-2015-04-20/sample-conll-file.txt  \n",
            "  inflating: stanford-ner-2015-04-20/sample.ner.txt  \n",
            "   creating: stanford-ner-2015-04-20/lib/\n",
            "  inflating: stanford-ner-2015-04-20/lib/jollyday-0.4.7.jar  \n",
            "  inflating: stanford-ner-2015-04-20/lib/joda-time.jar  \n",
            "  inflating: stanford-ner-2015-04-20/lib/stanford-ner-resources.jar  \n",
            "  inflating: stanford-ner-2015-04-20/ner-gui.command  \n",
            "  inflating: stanford-ner-2015-04-20/ner.sh  \n",
            "  inflating: stanford-ner-2015-04-20/NERDemo.java  \n",
            "  inflating: stanford-ner-2015-04-20/ner.bat  \n",
            "  inflating: stanford-ner-2015-04-20/stanford-ner-3.5.2-javadoc.jar  \n",
            "   creating: stanford-ner-2015-04-20/classifiers/\n",
            "  inflating: stanford-ner-2015-04-20/classifiers/english.conll.4class.distsim.prop  \n",
            "  inflating: stanford-ner-2015-04-20/classifiers/example.serialized.ncc.ncc.ser.gz  \n",
            "  inflating: stanford-ner-2015-04-20/classifiers/english.muc.7class.distsim.crf.ser.gz  \n",
            "  inflating: stanford-ner-2015-04-20/classifiers/english.conll.4class.distsim.crf.ser.gz  \n",
            "  inflating: stanford-ner-2015-04-20/classifiers/english.muc.7class.distsim.prop  \n",
            "  inflating: stanford-ner-2015-04-20/classifiers/english.all.3class.distsim.prop  \n",
            "  inflating: stanford-ner-2015-04-20/classifiers/example.serialized.ncc.prop  \n",
            "  inflating: stanford-ner-2015-04-20/classifiers/english.all.3class.distsim.crf.ser.gz  \n",
            "  inflating: stanford-ner-2015-04-20/stanford-ner-3.5.2-sources.jar  \n",
            "  inflating: stanford-ner-2015-04-20/sample.txt  \n",
            "  inflating: stanford-ner-2015-04-20/sample-w-time.txt  \n",
            "  inflating: stanford-ner-2015-04-20/ner-gui.sh  \n",
            "  inflating: stanford-ner-2015-04-20/LICENSE.txt  \n",
            "Archive:  stanford-parser-full-2015-04-20.zip\n",
            "   creating: stanford-parser-full-2015-04-20/\n",
            "  inflating: stanford-parser-full-2015-04-20/README.txt  \n",
            "   creating: stanford-parser-full-2015-04-20/data/\n",
            "  inflating: stanford-parser-full-2015-04-20/data/testsent.txt  \n",
            " extracting: stanford-parser-full-2015-04-20/data/chinese-onesent-unseg-gb18030.txt  \n",
            " extracting: stanford-parser-full-2015-04-20/data/english-onesent.txt  \n",
            " extracting: stanford-parser-full-2015-04-20/data/chinese-onesent-gb18030.txt  \n",
            " extracting: stanford-parser-full-2015-04-20/data/chinese-onesent-utf8.txt  \n",
            "  inflating: stanford-parser-full-2015-04-20/data/french-onesent.txt  \n",
            " extracting: stanford-parser-full-2015-04-20/data/arabic-onesent-utf8.txt  \n",
            "  inflating: stanford-parser-full-2015-04-20/data/chinese-onesent-unseg-utf8.txt  \n",
            " extracting: stanford-parser-full-2015-04-20/data/german-onesent.txt  \n",
            "  inflating: stanford-parser-full-2015-04-20/data/pos-sentences.txt  \n",
            " extracting: stanford-parser-full-2015-04-20/ejml-0.23-src.zip  \n",
            "  inflating: stanford-parser-full-2015-04-20/stanford-parser.jar  \n",
            "  inflating: stanford-parser-full-2015-04-20/build.xml  \n",
            "  inflating: stanford-parser-full-2015-04-20/lexparser-lang.sh  \n",
            "  inflating: stanford-parser-full-2015-04-20/pom.xml  \n",
            "  inflating: stanford-parser-full-2015-04-20/stanford-parser-3.5.2-models.jar  \n",
            "  inflating: stanford-parser-full-2015-04-20/DependencyParserDemo.java  \n",
            "  inflating: stanford-parser-full-2015-04-20/lexparser-gui.command  \n",
            "  inflating: stanford-parser-full-2015-04-20/stanford-parser-3.5.2-sources.jar  \n",
            "  inflating: stanford-parser-full-2015-04-20/README_dependencies.txt  \n",
            "  inflating: stanford-parser-full-2015-04-20/lexparser-lang-train-test.sh  \n",
            "   creating: stanford-parser-full-2015-04-20/bin/\n",
            "  inflating: stanford-parser-full-2015-04-20/bin/makeSerialized.csh  \n",
            "  inflating: stanford-parser-full-2015-04-20/bin/run-tb-preproc  \n",
            "  inflating: stanford-parser-full-2015-04-20/StanfordDependenciesManual.pdf  \n",
            "  inflating: stanford-parser-full-2015-04-20/lexparser.bat  \n",
            "  inflating: stanford-parser-full-2015-04-20/stanford-parser-3.5.2-javadoc.jar  \n",
            "   creating: stanford-parser-full-2015-04-20/conf/\n",
            "  inflating: stanford-parser-full-2015-04-20/conf/ftb-latest.conf  \n",
            "  inflating: stanford-parser-full-2015-04-20/conf/atb-latest.conf  \n",
            "  inflating: stanford-parser-full-2015-04-20/lexparser-gui.bat  \n",
            "  inflating: stanford-parser-full-2015-04-20/ejml-0.23.jar  \n",
            "  inflating: stanford-parser-full-2015-04-20/Makefile  \n",
            "  inflating: stanford-parser-full-2015-04-20/ParserDemo2.java  \n",
            "  inflating: stanford-parser-full-2015-04-20/lexparser_lang.def  \n",
            "  inflating: stanford-parser-full-2015-04-20/lexparser.sh  \n",
            "  inflating: stanford-parser-full-2015-04-20/ParserDemo.java  \n",
            "  inflating: stanford-parser-full-2015-04-20/lexparser-gui.sh  \n",
            "  inflating: stanford-parser-full-2015-04-20/ShiftReduceDemo.java  \n",
            "  inflating: stanford-parser-full-2015-04-20/LICENSE.txt  \n",
            "Archive:  stanford-postagger-full-2015-04-20.zip\n",
            "   creating: stanford-postagger-full-2015-04-20/\n",
            "  inflating: stanford-postagger-full-2015-04-20/README.txt  \n",
            "  inflating: stanford-postagger-full-2015-04-20/sample-input.txt  \n",
            "   creating: stanford-postagger-full-2015-04-20/data/\n",
            "  inflating: stanford-postagger-full-2015-04-20/data/enclitic-inflections.data  \n",
            "  inflating: stanford-postagger-full-2015-04-20/build.xml  \n",
            "  inflating: stanford-postagger-full-2015-04-20/stanford-postagger.sh  \n",
            "  inflating: stanford-postagger-full-2015-04-20/stanford-postagger-3.5.2.jar  \n",
            "  inflating: stanford-postagger-full-2015-04-20/stanford-postagger-gui.sh  \n",
            "  inflating: stanford-postagger-full-2015-04-20/stanford-postagger-3.5.2-javadoc.jar  \n",
            "  inflating: stanford-postagger-full-2015-04-20/stanford-postagger.jar  \n",
            "  inflating: stanford-postagger-full-2015-04-20/stanford-postagger.bat  \n",
            "  inflating: stanford-postagger-full-2015-04-20/sample-output.txt  \n",
            "  inflating: stanford-postagger-full-2015-04-20/stanford-postagger-3.5.2-sources.jar  \n",
            "  inflating: stanford-postagger-full-2015-04-20/TaggerDemo2.java  \n",
            "  inflating: stanford-postagger-full-2015-04-20/stanford-postagger-gui.bat  \n",
            "   creating: stanford-postagger-full-2015-04-20/models/\n",
            "  inflating: stanford-postagger-full-2015-04-20/models/german-fast-caseless.tagger  \n",
            "  inflating: stanford-postagger-full-2015-04-20/models/german-dewac.tagger  \n",
            "  inflating: stanford-postagger-full-2015-04-20/models/arabic-train.tagger.props  \n",
            "  inflating: stanford-postagger-full-2015-04-20/models/chinese-distsim.tagger  \n",
            "  inflating: stanford-postagger-full-2015-04-20/models/wsj-0-18-bidirectional-distsim.tagger  \n",
            "  inflating: stanford-postagger-full-2015-04-20/models/chinese-nodistsim.tagger.props  \n",
            "  inflating: stanford-postagger-full-2015-04-20/models/english-bidirectional-distsim.tagger.props  \n",
            "  inflating: stanford-postagger-full-2015-04-20/models/wsj-0-18-bidirectional-nodistsim.tagger  \n",
            "  inflating: stanford-postagger-full-2015-04-20/models/wsj-0-18-caseless-left3words-distsim.tagger  \n",
            "  inflating: stanford-postagger-full-2015-04-20/models/chinese-nodistsim.tagger  \n",
            "  inflating: stanford-postagger-full-2015-04-20/models/spanish.tagger  \n",
            "  inflating: stanford-postagger-full-2015-04-20/models/german-fast-caseless.tagger.props  \n",
            "  inflating: stanford-postagger-full-2015-04-20/models/README-Models.txt  \n",
            "  inflating: stanford-postagger-full-2015-04-20/models/english-caseless-left3words-distsim.tagger  \n",
            "  inflating: stanford-postagger-full-2015-04-20/models/french.tagger.props  \n",
            "  inflating: stanford-postagger-full-2015-04-20/models/wsj-0-18-left3words-distsim.tagger  \n",
            "  inflating: stanford-postagger-full-2015-04-20/models/english-bidirectional-distsim.tagger  \n",
            "  inflating: stanford-postagger-full-2015-04-20/models/spanish-distsim.tagger.props  \n",
            "  inflating: stanford-postagger-full-2015-04-20/models/english-left3words-distsim.tagger  \n",
            "  inflating: stanford-postagger-full-2015-04-20/models/wsj-0-18-left3words-nodistsim.tagger  \n",
            "  inflating: stanford-postagger-full-2015-04-20/models/wsj-0-18-left3words-nodistsim.tagger.props  \n",
            "  inflating: stanford-postagger-full-2015-04-20/models/wsj-0-18-left3words-distsim.tagger.props  \n",
            "  inflating: stanford-postagger-full-2015-04-20/models/english-caseless-left3words-distsim.tagger.props  \n",
            "  inflating: stanford-postagger-full-2015-04-20/models/arabic.tagger  \n",
            "  inflating: stanford-postagger-full-2015-04-20/models/arabic.tagger.props  \n",
            "  inflating: stanford-postagger-full-2015-04-20/models/german-fast.tagger  \n",
            "  inflating: stanford-postagger-full-2015-04-20/models/wsj-0-18-caseless-left3words-distsim.tagger.props  \n",
            "  inflating: stanford-postagger-full-2015-04-20/models/wsj-0-18-bidirectional-distsim.tagger.props  \n",
            "  inflating: stanford-postagger-full-2015-04-20/models/spanish-distsim.tagger  \n",
            "  inflating: stanford-postagger-full-2015-04-20/models/chinese-distsim.tagger.props  \n",
            "  inflating: stanford-postagger-full-2015-04-20/models/wsj-0-18-bidirectional-nodistsim.tagger.props  \n",
            "  inflating: stanford-postagger-full-2015-04-20/models/german-fast.tagger.props  \n",
            "  inflating: stanford-postagger-full-2015-04-20/models/english-left3words-distsim.tagger.props  \n",
            "  inflating: stanford-postagger-full-2015-04-20/models/french.tagger  \n",
            "  inflating: stanford-postagger-full-2015-04-20/models/german-hgc.tagger.props  \n",
            "  inflating: stanford-postagger-full-2015-04-20/models/german-dewac.tagger.props  \n",
            "  inflating: stanford-postagger-full-2015-04-20/models/arabic-train.tagger  \n",
            "  inflating: stanford-postagger-full-2015-04-20/models/german-hgc.tagger  \n",
            "  inflating: stanford-postagger-full-2015-04-20/models/spanish.tagger.props  \n",
            "  inflating: stanford-postagger-full-2015-04-20/TaggerDemo.java  \n",
            "  inflating: stanford-postagger-full-2015-04-20/LICENSE.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qH06FxVHAhr1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv ./stanford-ner-2015-04-20 ./stanford\n",
        "!mv ./stanford-parser-full-2015-04-20 ./stanford\n",
        "!mv ./stanford-postagger-full-2015-04-20 ./stanford\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bvBwithDJFz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the CORENLP_HOME environment variable to point to the installation location\n",
        "import os\n",
        "os.environ[\"'STANFORD_PARSER'\"] = \"./stanford/jars\"\n",
        "os.environ['STANFORD_MODELS'] = './standford/jars'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJt8ocoXHZMf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 835
        },
        "outputId": "4890e6a0-a93f-4297-9297-dc14a36f9fe5"
      },
      "source": [
        "!pip install textacy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting textacy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/5e/3b8391cf6ff39350b73f8421184cf6792002b5c2c17982b7c9fbd5ff36de/textacy-0.9.1-py3-none-any.whl (203kB)\n",
            "\r\u001b[K     |█▋                              | 10kB 20.3MB/s eta 0:00:01\r\u001b[K     |███▏                            | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |████▉                           | 30kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |████████                        | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 61kB 2.5MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 71kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 81kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 92kB 3.7MB/s eta 0:00:01\r\u001b[K     |████████████████                | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 204kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.0.12 in /usr/local/lib/python3.6/dist-packages (from textacy) (2.1.9)\n",
            "Requirement already satisfied: scikit-learn>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (0.21.3)\n",
            "Requirement already satisfied: srsly>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from textacy) (0.2.0)\n",
            "Requirement already satisfied: joblib>=0.13.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (0.14.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (1.3.2)\n",
            "Requirement already satisfied: cachetools>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from textacy) (3.1.1)\n",
            "Collecting pyphen>=0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/82/08a3629dce8d1f3d91db843bb36d4d7db6b6269d5067259613a0d5c8a9db/Pyphen-0.9.5-py2.py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 42.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (2.4)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (2.21.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (1.17.4)\n",
            "Requirement already satisfied: tqdm>=4.19.6 in /usr/local/lib/python3.6/dist-packages (from textacy) (4.28.1)\n",
            "Collecting jellyfish>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/80/bcacc7affb47be7279d7d35225e1a932416ed051b315a7f9df20acf04cbe/jellyfish-0.7.2.tar.gz (133kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 46.3MB/s \n",
            "\u001b[?25hCollecting cytoolz>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/b1/7f16703fe4a497879b1b457adf1e472fad2d4f030477698b16d2febf38bb/cytoolz-0.10.1.tar.gz (475kB)\n",
            "\u001b[K     |████████████████████████████████| 481kB 38.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyemd>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (0.5.1)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.12->textacy) (7.0.8)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.12->textacy) (1.0.2)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.12->textacy) (0.2.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.12->textacy) (2.0.3)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.12->textacy) (0.9.6)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.12->textacy) (2.0.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.12->textacy) (0.4.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->textacy) (4.4.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->textacy) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->textacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->textacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->textacy) (2019.9.11)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz>=0.8.0->textacy) (0.10.0)\n",
            "Building wheels for collected packages: jellyfish, cytoolz\n",
            "  Building wheel for jellyfish (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jellyfish: filename=jellyfish-0.7.2-cp36-cp36m-linux_x86_64.whl size=73004 sha256=50da9a377af9c276c76d174628d9c8482b5825e329f396ff63384ab0edb97fc0\n",
            "  Stored in directory: /root/.cache/pip/wheels/e8/fe/99/d8fa8f2ef7b82a625b0b77a84d319b0b50693659823c4effb4\n",
            "  Building wheel for cytoolz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cytoolz: filename=cytoolz-0.10.1-cp36-cp36m-linux_x86_64.whl size=1256647 sha256=3d711e5ac0875ea43d186fb5cf93fdad38f679556f9410490afc820198257a49\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/2a/18/d962b614e055577e7d9a3e4813e0742f822ca9c8800cc3783a\n",
            "Successfully built jellyfish cytoolz\n",
            "Installing collected packages: pyphen, jellyfish, cytoolz, textacy\n",
            "Successfully installed cytoolz-0.10.1 jellyfish-0.7.2 pyphen-0.9.5 textacy-0.9.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFmPwqst-slD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "22dedb97-acbc-4e04-f091-8db364bd9582"
      },
      "source": [
        "#import spacy\n",
        "import textacy\n",
        "SUBJ = [\"nsubj\",\"nsubjpass\"] \n",
        "VERB = [\"ROOT\"] \n",
        "OBJ = [\"dobj\", \"pobj\", \"dobj\"] \n",
        "text = nlp(u'The cat sat on the mat. The cat jumped and picked up the biscuit. The cat ate biscuit and cookies.')\n",
        "sub_toks = [tok for tok in text if (tok.dep_ in SUBJ) ]\n",
        "obj_toks = [tok for tok in text if (tok.dep_ in OBJ) ]\n",
        "vrb_toks = [tok for tok in text if (tok.dep_ in VERB) ]\n",
        "text_ext = list(textacy.extract.subject_verb_object_triples(text))\n",
        "print(\"Subjects:\", sub_toks)\n",
        "print(\"VERB :\", vrb_toks)\n",
        "print(\"OBJECT(s):\", obj_toks)\n",
        "print (\"SVO:\", text_ext)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Subjects: [cat, cat, cat]\n",
            "VERB : [sat, jumped, ate]\n",
            "OBJECT(s): [mat, biscuit, biscuit]\n",
            "SVO: [(cat, ate, biscuit), (cat, ate, cookies)]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}